# -*- coding: utf-8 -*-
"""Magicbricks_webscrapping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-y42UH7z5BUB3ly5PBx05g5sgAf3iJQ6
"""

!pip install selenium
!pip install webdriver-manager

import numpy as np

"""### Real time selenium based script for 1 bhk flat data extraction in Bangalore from magicbricks website"""

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import pandas as pd
import time

def get_text(element):
    if element:
        return element.get_text(strip=True)
    return "N/A"

# URL for flats in Bangalore
BASE_URL = "https://www.magicbricks.com/property-for-sale/residential-real-estate?bedroom=1&proptype=Multistorey-Apartment,Builder-Floor-Apartment,Penthouse,Studio-Apartment&cityName=Bangalore"

# Setup browser
opts = Options()
opts.add_argument("--headless")
opts.add_argument("--no-sandbox")
opts.add_argument("--disable-dev-shm-usage")
opts.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")

driver = webdriver.Chrome(options=opts)

all_rows = []
page = 1

try:
    while True:
        # Build URL with page number
        if page == 1:
            url = BASE_URL
        else:
            url = BASE_URL + "&page=" + str(page)

        print("\n--- Scraping Page " + str(page) + " ---")
        print("URL: " + url)

        driver.get(url)
        time.sleep(8)

        # Scroll to load all properties on current page
        print("Scrolling to load properties...")
        last_height = driver.execute_script("return document.body.scrollHeight")

        scroll_attempts = 0
        while scroll_attempts < 5:
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3)

            new_height = driver.execute_script("return document.body.scrollHeight")

            if new_height == last_height:
                break

            last_height = new_height
            scroll_attempts = scroll_attempts + 1

        time.sleep(2)

        soup = BeautifulSoup(driver.page_source, "lxml")
        cards = soup.select("div.mb-srp__card")

        print("Found " + str(len(cards)) + " property cards on this page")

        # If no cards found, we've reached the end
        if len(cards) == 0:
            print("No more properties found. Stopping.")
            break

        # Extract data from each card
        for idx, card in enumerate(cards, 1):
            try:
                # Extract BHK
                bhk_elem = card.select_one("span.mb-srp__card--highlight")
                bhk = get_text(bhk_elem)

                # Extract title
                title_elem = card.select_one("h2.mb-srp__card--title")
                title = get_text(title_elem)

                # Extract locality
                locality_elem = card.select_one("span.mb-srp__card__ads--location__link")
                locality = get_text(locality_elem)

                # Extract amount
                amount_elem = card.select_one("div.mb-srp__card__price--amount")
                amount = get_text(amount_elem)

                # Extract floor
                floor = "N/A"
                floor_elem = card.select_one("div.mb-srp__card__summary__list")
                if floor_elem:
                    floor_text = get_text(floor_elem)
                    if "Floor" in floor_text:
                        parts = floor_text.split("|")
                        for part in parts:
                            if "Floor" in part:
                                floor = part.strip()
                                break

                # Extract status
                status = "N/A"
                summary = card.select_one("div.mb-srp__card__summary__list")
                if summary:
                    summary_text = get_text(summary)
                    if "Ready to Move" in summary_text:
                        status = "Ready to Move"
                    elif "Under Construction" in summary_text:
                        status = "Under Construction"

                # Extract builder name
                builder_elem = card.select_one("span.mb-srp__card__ads--name")
                builder_name = get_text(builder_elem)

                all_rows.append({
                    "Page": page,
                    "BHK": bhk,
                    "Title": title,
                    "Locality": locality,
                    "Amount": amount,
                    "Floor": floor,
                    "Status": status,
                    "Builder_Name": builder_name
                })

            except Exception as e:
                print("Error on card " + str(idx) + ": " + str(e))
                continue

        print("Processed " + str(len(cards)) + " properties from page " + str(page))
        print("Total properties so far: " + str(len(all_rows)))

        # Move to next page
        page = page + 1

        # Add delay between pages
        time.sleep(3)

        # Safety limit - stop after 100 pages (or adjust as needed)
        if page > 10:
            print("\nReached page limit. Stopping.")
            break

except Exception as e:
    print(f"An error occurred during scraping: {e}")

finally:
    driver.quit()

# Create DataFrame
df = pd.DataFrame(all_rows)

"""### DataFrame formation"""

df = pd.DataFrame(all_rows)

df.duplicated()
df.drop_duplicates(inplace=True)  # Modifies original dataframe

"""### BHK extration from the Title column"""

def function(text):
    if isinstance(text, str):
        text = text.split()[0]  # Remove str() conversion
    return text

df["BHK"] = df["Title"].apply(function)
df.head(4)

"""###  Extraction of location from title"""

df["Locality"] = df["Title"].str.split(" in ").str[1].str.split(",").str[0]
df.head(5)

"""### Converting N/A to NaN for easy calculation"""

# Only process text columns
for column in df.select_dtypes(include='object').columns:
    df[column] = df[column].replace("N/A", np.nan)

df.head(4)

"""### Area extraction from the floor column text"""

def area(text):
  if isinstance(text, str):
    text=text.split()[1]
  return text
df["Area"]=df["Floor"].apply(area)
df["Area"]

df["Area_sqft"] = df["Area"].str.replace("Area", "").str.strip()
df["Area_sqft"] = pd.to_numeric(df["Area_sqft"], errors='coerce')

# Drop rows where Area_sqft is NaN
df = df.dropna(subset=['Area_sqft'])
df["Area_sqft"] = df["Area_sqft"].astype(int)

df["Area_sqft"]

"""### removing not known column"""

df.drop(columns=["Builder_Name"], inplace=True)
df.head(2)

def convert_amount(text):
    if isinstance(text, str):
        # Remove ‚Çπ symbol and spaces
        text = text.replace('‚Çπ', '').strip()

        # Extract the number (everything before Lac/Cr)
        number = float(text.split()[0])

        # Check unit and multiply
        if 'Lac' in text:
            return number * 100000
        elif 'Cr' in text:
            return number * 10000000
        else:
            return number
    return None

df["Amount"] = df["Amount"].apply(convert_amount)
df["Amount"]

"""### removing NaN values"""

df.dropna(inplace=True)
df

"""### Cleaned data stored in excel file"""

df.to_excel("1bhk_bangalore.xlsx", index=False)

"""### Data Visualization"""

sns.histplot(x=df["Amount"], bins=40)
plt.show()

round(df["Amount"].median(),2)   # median 6500000.0
round(df["Amount"].mode()[0],2)  #mode 3000000.0
round(df["Amount"].mean(),2)    # mean 7330095.51

"""### Distribution of Amount and Area"""

sns.jointplot(data=df, x="Amount", y="Area_sqft", kind="reg", color="coral")
plt.show()

"""### Top 5 common locality"""

df["Locality"].value_counts()[:5]

df.groupby("Locality")[["Amount", "Area_sqft"]].transform("mean").sort_values(by=["Amount"], ascending=False)[:5]

"""### Top 20 localities"""

import seaborn as sns
import matplotlib.pyplot as plt

# Get mean values
means = df.groupby("Locality")[["Amount", "Area_sqft"]].mean().sort_values(by="Amount", ascending=False).reset_index().head(20)


# Seaborn barplot
plt.figure(figsize=(16,7))
sns.barplot( x=means["Locality"], y=means["Amount"], palette="viridis")
plt.xticks(rotation=45, ha='right')
plt.title("Top 20 Localities by Average Amount")
plt.xlabel("Locality")
plt.ylabel("Average Amount (‚Çπ)")
plt.tight_layout()
plt.show()

means = df.groupby("Locality")[["Amount", "Area_sqft"]].mean().sort_values(by="Area_sqft", ascending=False).reset_index().head(20)


# Seaborn barplot
plt.figure(figsize=(16,7))
sns.barplot( x=means["Locality"], y=means["Area_sqft"], palette="viridis")
plt.xticks(rotation=45, ha='right')
plt.title("Top 20 Localities by Average Area_sqft")
plt.xlabel("Locality")
plt.ylabel("Average Area sqft")
plt.tight_layout()
plt.show()

"""### Correlation between Amount and Area_sqft"""

correlation = df[["Amount", "Area_sqft"]].corr()

plt.figure(figsize=(6, 5))
sns.heatmap(correlation, annot=True, cmap="coolwarm")
plt.title("Correlation: Amount vs Area_sqft")
plt.tight_layout()
plt.show()

"""### Mean values by Locality"""

locality_means = df.groupby("Locality")[["Amount", "Area_sqft"]].mean().sort_values(by="Amount", ascending=False).head(20)

# Create heatmap
plt.figure(figsize=(10, 10))
sns.heatmap(locality_means.values, annot=True, fmt='.0f', cmap="YlOrRd", linewidths=0.5, yticklabels=locality_means.index)
plt.title("Heatmap: Amount & Area by Top 20 Localities")
plt.ylabel("Locality")
plt.xlabel("Metrics")
plt.tight_layout()
plt.show()

"""# üè† MagicBricks Real Estate Web Scraping & Visualization Project

**üìÖ Created on:** 2025-11-02  
**üë®‚Äçüíª Author:** Sourav Mondal

---

## üìò Project Overview

This project demonstrates **end-to-end web scraping, cleaning, and visualization** of property listings from [**MagicBricks**](https://www.magicbricks.com).

It uses **Selenium** for browser automation, **BeautifulSoup** for HTML parsing, **Pandas** for data cleaning, and **Matplotlib/Seaborn** for visualization.

The notebook automates browser scrolling, scrapes multiple pages, extracts property details (price, title, location, etc.), cleans and saves data to Excel/CSV, and visualizes key insights like **price distribution**, **price vs. area**, and **top localities**.

---

## üß† Tech Stack & Libraries Used

- **selenium** - Browser automation and dynamic page handling
- **webdriver-manager** - Auto-handling of ChromeDriver
- **beautifulsoup4** - Parsing HTML content
- **pandas** - Data cleaning and analysis
- **matplotlib** - Static visualizations
- **seaborn** - Statistical visualizations
- **numpy** - Numerical operations
- **time** - Handling dynamic waits

---

## ‚öôÔ∏è Installation & Setup

Run these commands in your environment or notebook before execution:
```bash
pip install selenium webdriver-manager beautifulsoup4 pandas matplotlib seaborn plotly openpyxl numpy
```

---

## üöÄ Workflow Summary

1. **Set Up WebDriver:** Initialize Selenium with Chrome in headless mode
2. **Scrape Listings:** Automate scrolling, pagination, and card extraction
3. **Extract Data:** Parse titles, prices, locations, details, and URLs using BeautifulSoup
4. **Clean Data:** Remove duplicates, convert price/area to numeric, handle missing values
5. **Store Output:** Save cleaned data to CSV and Excel files
6. **Visualize Insights:** Use Matplotlib & Seaborn for trend and distribution charts

---

## üìÅ Project Structure
```
MagicBricks_WebScraping/
‚îÇ
‚îú‚îÄ‚îÄ Magicbricks_webscrapping.ipynb   # Main notebook file
‚îú‚îÄ‚îÄ magicbricks_data.csv             # Cleaned data (output)
‚îú‚îÄ‚îÄ magicbricks_data.xlsx            # Excel version of output
‚îú‚îÄ‚îÄ README.md                        # This file
‚îî‚îÄ‚îÄ requirements.txt                 # Dependencies (optional)
```

---

## üßπ Data Cleaning Process

- Removed unwanted text symbols like ‚Çπ, commas, and unit words
- Converted price and area to numeric using regex extraction
- Trimmed whitespace and standardized text case
- Handled missing values and duplicates

---

## üíæ Data Output Schema

**Column Descriptions:**

- **Title** - Property title or headline
- **Price** - Listed price (string)
- **Price_num** - Numeric price value (converted)
- **Location** - Area / neighborhood
- **Details** - BHK, size, furnishing, etc.
- **URL** - Direct property listing link
- **Scraped_at** - Timestamp of data collection

---

## üßæ Insights Gained

- Larger area properties tend to show an exponential increase in price
- Median prices vary drastically between localities like **Whitefield**, **Indiranagar**, and **Koramangala**
- Price distribution is **right-skewed** ‚Äî most listings fall within affordable/mid-range, with a few luxury outliers
- Log-scaling reveals a smoother normal distribution of property values

---

## üí° Future Enhancements

- Add **BHK-level segmentation** (e.g., 1BHK vs 3BHK)
- Integrate **geolocation mapping** using Folium or Plotly Mapbox
- Automate daily scrapes with scheduling (cron / Airflow)
- Push data to **MySQL or MongoDB** for long-term tracking
- Deploy a **Streamlit dashboard** for real-time visualization

---

## ‚ö†Ô∏è Ethical & Legal Notice

This project is for **educational and research purposes only.**

Always comply with MagicBricks' **Terms of Service** and **robots.txt** before scraping.

Avoid aggressive scraping (use reasonable delays, limited pages).

---

## üßë‚Äçüíª Author Information

**Sourav Mondal**

üìß souravmondal5f@gmail.com

üîó [LinkedIn Profile](https://www.linkedin.com/in/sourav-mondal-7991b5373/)

---

## üìÑ License

This project is open source and available for educational purposes.

---


"""

